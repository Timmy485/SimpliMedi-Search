{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOMER_ID = os.environ.get(\"CUSTOMER_ID\")\n",
    "CORPUS_ID = os.environ.get(\"CORPUS_ID\")\n",
    "API_KEY = os.environ.get(\"API_KEY\")\n",
    "AUTH_URL = os.environ.get(\"AUTH_URL\")\n",
    "APP_CLIENT_ID = os.environ.get(\"APP_CLIENT_ID\")\n",
    "APP_CLIENT_SECRET = os.environ.get(\"APP_CLIENT_SECRET\")\n",
    "IDX_ADDRESS = os.environ.get(\"IDX_ADDRESS\")\n",
    "TONIC_VALIDATE_API_KEY = os.environ.get(\"TONIC_VALIDATE_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_create_corpus_json():\n",
    "    \"\"\"Returns a create corpus json.\"\"\"\n",
    "    corpus = {\n",
    "        \"name\": \"Vectara Test Corpus(Python)\",\n",
    "        \"description\": \"An example corpus generated via REST API from Python code.\",\n",
    "    }\n",
    "    return json.dumps({\"corpus\": corpus})\n",
    "\n",
    "\n",
    "def create_corpus(customer_id: int, admin_address: str, jwt_token: str):\n",
    "    \"\"\"Create a corpus.\n",
    "    Args:\n",
    "        customer_id: Unique customer ID in vectara platform.\n",
    "        admin_address: Address of the admin server. e.g., api.vectara.io\n",
    "        jwt_token: A valid Auth token.\n",
    "\n",
    "    Returns:\n",
    "        (response, True) in case of success and returns (error, False) in case of failure.\n",
    "    \"\"\"\n",
    "\n",
    "    post_headers = {\n",
    "        \"customer-id\": f\"{customer_id}\",\n",
    "        \"Authorization\": f\"Bearer {jwt_token}\",\n",
    "    }\n",
    "    response = requests.post(\n",
    "        f\"https://{admin_address}/v1/create-corpus\",\n",
    "        data=_get_create_corpus_json(),\n",
    "        verify=True,\n",
    "        headers=post_headers,\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        logging.error(\n",
    "            \"Create Corpus failed with code %d, reason %s, text %s\",\n",
    "            response.status_code,\n",
    "            response.reason,\n",
    "            response.text,\n",
    "        )\n",
    "        return response, False\n",
    "\n",
    "    message = response.json()\n",
    "    if message[\"status\"] and message[\"status\"][\"code\"] != \"OK\":\n",
    "        logging.error(\"Create Corpus failed with status: %s\", message[\"status\"])\n",
    "        return message[\"status\"], False\n",
    "\n",
    "    return message, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_delete_corpus_json(customer_id: int, corpus_id: int):\n",
    "    \"\"\"Returns a delete corpus JSON.\"\"\"\n",
    "    corpus = {\n",
    "        \"customer_id\": customer_id,\n",
    "        \"corpus_id\": corpus_id,\n",
    "    }\n",
    "\n",
    "    return json.dumps(corpus)\n",
    "\n",
    "\n",
    "def delete_corpus(customer_id: int, corpus_id: int, admin_address: str, jwt_token: str):\n",
    "    \"\"\"Deletes a corpus.\n",
    "\n",
    "    Args:\n",
    "        customer_id: Unique customer ID in vectara platform.\n",
    "        corpus_id: Corpus ID in vectara platform.\n",
    "        admin_address: Address of the admin server. e.g., api.vectara.io\n",
    "        jwt_token: A valid Auth token.\n",
    "\n",
    "    Returns:\n",
    "        (response, True) in case of success and returns (error, False) in case of failure.\n",
    "    \"\"\"\n",
    "    post_headers = {\n",
    "        \"customer-id\": f\"{customer_id}\",\n",
    "        \"Authorization\": f\"Bearer {jwt_token}\",\n",
    "    }\n",
    "    response = requests.post(\n",
    "        f\"https://{admin_address}/v1/delete-corpus\",\n",
    "        data=_get_delete_corpus_json(customer_id, corpus_id),\n",
    "        verify=True,\n",
    "        headers=post_headers,\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        logging.error(\n",
    "            \"Delete Corpus failed with code %d, reason %s, text %s\",\n",
    "            response.status_code,\n",
    "            response.reason,\n",
    "            response.text,\n",
    "        )\n",
    "        return response, False\n",
    "\n",
    "    message = response.json()\n",
    "    if message[\"status\"] and message[\"status\"][\"code\"] != \"OK\":\n",
    "        logging.error(\"Delete Corpus failed with status: %s\", message.status)\n",
    "        return message.status, False\n",
    "\n",
    "    return message, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_reset_corpus_json(customer_id: int, corpus_id: int):\n",
    "    \"\"\"Returns a reset corpus json.\"\"\"\n",
    "    corpus = {\n",
    "        \"customer_id\": customer_id,\n",
    "        \"corpus_id\": corpus_id,\n",
    "    }\n",
    "\n",
    "    return json.dumps(corpus)\n",
    "\n",
    "\n",
    "def reset_corpus(customer_id: int, corpus_id: int, admin_address: str, jwt_token: str):\n",
    "    \"\"\"Reset a corpus.\n",
    "    Args:\n",
    "        customer_id: Unique customer ID in vectara platform.\n",
    "        corpus_id: Corpus ID in vectara platform.\n",
    "        admin_address: Address of the admin server. e.g., api.vectara.io\n",
    "        jwt_token: A valid Auth token.\n",
    "\n",
    "    Returns:\n",
    "        (response, True) in case of success and returns (error, False) in case of failure.\n",
    "    \"\"\"\n",
    "\n",
    "    post_headers = {\n",
    "        \"customer-id\": f\"{customer_id}\",\n",
    "        \"Authorization\": f\"Bearer {jwt_token}\",\n",
    "    }\n",
    "    response = requests.post(\n",
    "        f\"https://{admin_address}/v1/reset-corpus\",\n",
    "        data=_get_reset_corpus_json(customer_id, corpus_id),\n",
    "        verify=True,\n",
    "        headers=post_headers,\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        logging.error(\n",
    "            \"Reset Corpus failed with code %d, reason %s, text %s\",\n",
    "            response.status_code,\n",
    "            response.reason,\n",
    "            response.text,\n",
    "        )\n",
    "        return response, False\n",
    "\n",
    "    message = response.json()\n",
    "    if message[\"status\"] and message[\"status\"][\"code\"] != \"OK\":\n",
    "        logging.error(\"Delete Corpus failed with status: %s\", message.status)\n",
    "        return message.status, False\n",
    "\n",
    "    return message, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jwt_token():\n",
    "    \"\"\"Get JWT token from authentication service.\"\"\"\n",
    "    auth_url = AUTH_URL\n",
    "    client_id = APP_CLIENT_ID\n",
    "    client_secret = APP_CLIENT_SECRET\n",
    "\n",
    "    data = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_id\": client_id,\n",
    "        \"client_secret\": client_secret,\n",
    "    }\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "\n",
    "    response = requests.post(auth_url, headers=headers, data=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        response_data = response.json()\n",
    "        return response_data.get(\"access_token\")\n",
    "    else:\n",
    "        print(\"Error:\", response.text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(\n",
    "    customer_id: int, corpus_id: int, idx_address: str, jwt_token: str, file_path: str\n",
    "):\n",
    "    \"\"\"Uploads a file to the corpus.\n",
    "\n",
    "    Args:\n",
    "        customer_id: Unique customer ID in vectara platform.\n",
    "        corpus_id: ID of the corpus to which data needs to be indexed.\n",
    "        idx_address: Address of the indexing server. e.g., api.vectara.io\n",
    "        jwt_token: A valid Auth token.\n",
    "        file_path: Path to the file to be uploaded.\n",
    "\n",
    "    Returns:\n",
    "        (response, True) in case of success and returns (error, False) in case of failure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract filename from the file path\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    post_headers = {\"Authorization\": f\"Bearer {jwt_token}\"}\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        response = requests.post(\n",
    "            f\"https://{idx_address}/v1/upload?c={customer_id}&o={corpus_id}\",\n",
    "            files={\"file\": (file.name, file, \"application/octet-stream\")},\n",
    "            data={\"doc_metadata\": f'{{\"filename\": \"{filename}\"}}'},\n",
    "            verify=True,\n",
    "            headers=post_headers,\n",
    "        )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        logging.error(\n",
    "            \"REST upload failed with code %d, reason %s, text %s\",\n",
    "            response.status_code,\n",
    "            response.reason,\n",
    "            response.text,\n",
    "        )\n",
    "        return response, False\n",
    "\n",
    "    message = response.json()[\"response\"]\n",
    "    # An empty status indicates success.\n",
    "    if message[\"status\"] and message[\"status\"][\"code\"] not in (\"OK\", \"ALREADY_EXISTS\"):\n",
    "        logging.error(\"REST upload failed with status: %s\", message[\"status\"])\n",
    "        return message[\"status\"], False\n",
    "\n",
    "    return message, True\n",
    "\n",
    "\n",
    "def upload_files_in_directory(\n",
    "    customer_id: int, corpus_id: int, idx_address: str, directory_path: str\n",
    "):\n",
    "    \"\"\"Uploads all files in a directory to the corpus.\n",
    "\n",
    "    Args:\n",
    "        customer_id: Unique customer ID in Vectara platform.\n",
    "        corpus_id: ID of the corpus to which data needs to be indexed.\n",
    "        idx_address: Address of the indexing server. e.g., api.vectara.io\n",
    "        directory_path: Path to the directory containing files to be uploaded.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples containing (response, success) for each file upload.\n",
    "    \"\"\"\n",
    "    jwt_token = get_jwt_token()\n",
    "    if not jwt_token:\n",
    "        return []\n",
    "\n",
    "    file_uploads = []\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            response, success = upload_file(\n",
    "                customer_id, corpus_id, idx_address, jwt_token, file_path\n",
    "            )\n",
    "            file_uploads.append((response, success))\n",
    "    return file_uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_files_in_directory(\n",
    "    customer_id=CUSTOMER_ID,\n",
    "    corpus_id=CORPUS_ID,\n",
    "    idx_address=\"api.vectara.io\",\n",
    "    directory_path=\"corpus\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_temp_dir = os.path.abspath(\"temp\")\n",
    "upload_files_in_directory(\n",
    "    customer_id=CUSTOMER_ID,\n",
    "    corpus_id=CORPUS_ID,\n",
    "    idx_address=\"api.vectara.io\",\n",
    "    directory_path=abs_temp_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping language names to their lowercase initials\n",
    "language_initials = {\n",
    "    \"English\": \"eng\",\n",
    "    \"German\": \"deu\",\n",
    "    \"French\": \"fra\",\n",
    "    \"Chinese\": \"zho\",\n",
    "    \"Korean\": \"kor\",\n",
    "    \"Arabic\": \"ara\",\n",
    "    \"Russian\": \"rus\",\n",
    "    \"Thai\": \"tha\",\n",
    "    \"Dutch\": \"nld\",\n",
    "    \"Italian\": \"ita\",\n",
    "    \"Portuguese\": \"por\",\n",
    "    \"Spanish\": \"spa\",\n",
    "    \"Japanese\": \"jpn\",\n",
    "    \"Polish\": \"pol\",\n",
    "    \"Turkish\": \"tur\",\n",
    "    \"Vietnamese\": \"vie\",\n",
    "    \"Indonesian\": \"ind\",\n",
    "    \"Czech\": \"ces\",\n",
    "    \"Ukrainian\": \"ukr\",\n",
    "    \"Greek\": \"ell\",\n",
    "    \"Hebrew\": \"heb\",\n",
    "    \"Farsi/Persian\": \"fas\",\n",
    "    \"Hindi\": \"hin\",\n",
    "    \"Urdu\": \"urd\",\n",
    "    \"Swedish\": \"swe\",\n",
    "    \"Bengali\": \"ben\",\n",
    "    \"Malay\": \"msa\",\n",
    "    \"Romanian\": \"ron\",\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"GPT-3.5-Turbo\": \"vectara-summary-ext-v1.2.0\",\n",
    "    \"GPT-4-Turbo\": \"vectara-summary-ext-v1.3.0\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_query_json(\n",
    "    customer_id: int,\n",
    "    corpus_id: int,\n",
    "    query_value: str,\n",
    "    summarizer_prompt_name,\n",
    "    response_lang,\n",
    "    top_k=5,\n",
    "    max_summarized_results=10,\n",
    "    lambda_val=0.025,\n",
    "):\n",
    "    \"\"\"Returns a query JSON.\"\"\"\n",
    "    query = {\n",
    "        \"query\": [\n",
    "            {\n",
    "                \"query\": query_value,\n",
    "                \"num_results\": top_k,\n",
    "                \"corpus_key\": [\n",
    "                    {\n",
    "                        \"customer_id\": customer_id,\n",
    "                        \"corpus_id\": corpus_id,\n",
    "                        \"lexicalInterpolationConfig\": {\"lambda\": lambda_val},\n",
    "                    }\n",
    "                ],\n",
    "                \"summary\": [\n",
    "                    {\n",
    "                        \"summarizerPromptName\": summarizer_prompt_name,  # vectara-summary-ext-v1.2.0 (gpt-3.5-turbo) vectara-summary-ext-v1.3.0 (gpt-4.0)\n",
    "                        \"responseLang\": response_lang,  # auto to auto-detect\n",
    "                        \"maxSummarizedResults\": max_summarized_results,\n",
    "                        \"factual_consistency_score\": True,\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "    return json.dumps(query)\n",
    "\n",
    "\n",
    "def query_corpus(\n",
    "    customer_id: int, corpus_id: int, query_address: str, jwt_token: str, query: str\n",
    "):\n",
    "    \"\"\"Queries the data.\n",
    "\n",
    "    Args:\n",
    "        customer_id: Unique customer ID in vectara platform.\n",
    "        corpus_id: ID of the corpus to which data needs to be indexed.\n",
    "        query_address: Address of the querying server. e.g., api.vectara.io\n",
    "        jwt_token: A valid Auth token.\n",
    "\n",
    "    Returns:\n",
    "        (response, True) in case of success and returns (error, False) in case of failure.\n",
    "\n",
    "    \"\"\"\n",
    "    post_headers = {\n",
    "        \"customer-id\": f\"{customer_id}\",\n",
    "        \"Authorization\": f\"Bearer {jwt_token}\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"https://{query_address}/v1/query\",\n",
    "        data=_get_query_json(\n",
    "            customer_id,\n",
    "            corpus_id,\n",
    "            query,\n",
    "            models[\"GPT-4-Turbo\"],\n",
    "            language_initials[\"English\"],\n",
    "        ),\n",
    "        verify=True,\n",
    "        headers=post_headers,\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        logging.error(\n",
    "            \"Query failed with code %d, reason %s, text %s\",\n",
    "            response.status_code,\n",
    "            response.reason,\n",
    "            response.text,\n",
    "        )\n",
    "        return response, False\n",
    "\n",
    "    message = response.json()\n",
    "    if message[\"status\"] and any(\n",
    "        status[\"code\"] != \"OK\" for status in message[\"status\"]\n",
    "    ):\n",
    "        logging.error(\"Query failed with status: %s\", message[\"status\"])\n",
    "        return message[\"status\"], False\n",
    "\n",
    "    responses = message[\"responseSet\"][0][\"response\"]\n",
    "    documents = message[\"responseSet\"][0][\"document\"]\n",
    "    summary = message[\"responseSet\"][0][\"summary\"][0][\"text\"]\n",
    "    factual_consistency_score = message[\"responseSet\"][0][\"summary\"][0][\n",
    "        \"factualConsistency\"\n",
    "    ][\"score\"]\n",
    "\n",
    "    res = [[r[\"text\"], r[\"score\"]] for r in responses]\n",
    "    return res, summary, factual_consistency_score, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_txt = \"which was John Doe's condition?\"\n",
    "results, summary, score, documents = query_corpus(\n",
    "    CUSTOMER_ID, CORPUS_ID, IDX_ADDRESS, get_jwt_token(), query_txt\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query With LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "os.environ[\"VECTARA_CUSTOMER_ID\"] = os.environ.get(\"CUSTOMER_ID\")\n",
    "os.environ[\"VECTARA_CORPUS_ID\"] = \"6\"\n",
    "os.environ[\"VECTARA_API_KEY\"] = os.environ.get(\"CORPUS_6_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n",
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.indices.managed.vectara import VectaraIndex\n",
    "\n",
    "index = VectaraIndex()\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VECTARA_CORPUS_ID\"] = \"7\"\n",
    "os.environ[\"VECTARA_API_KEY\"] = os.environ.get(\"CORPUS_7_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.indices.managed.vectara import VectaraIndex\n",
    "\n",
    "documents = SimpleDirectoryReader(\"corpus\").load_data()\n",
    "index = VectaraIndex().from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an answer to the query based on the content of the essay\n",
    "response = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "answer = response.query(\"What was John Doe's condition?\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate With Tonic Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonic_validate import ValidateScorer, Benchmark, LLMResponse, ValidateApi\n",
    "from tonic_validate.metrics import (\n",
    "    AnswerSimilarityMetric,\n",
    "    RetrievalPrecisionMetric,\n",
    "    AugmentationPrecisionMetric,\n",
    "    AugmentationAccuracyMetric,\n",
    "    AnswerConsistencyMetric,\n",
    ")\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonic_validate import ValidateApi\n",
    "\n",
    "TONIC_VALIDATE_API_KEY = os.environ.get(\"TONIC_VALIDATE_API_KEY\")\n",
    "validate_api = ValidateApi(TONIC_VALIDATE_API_KEY)\n",
    "benchmark = validate_api.get_benchmark(\"fff9af14-643e-4bfd-b565-e8761c1dca88\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring responses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving responses: 100%|██████████| 21/21 [01:07<00:00,  3.22s/it]\n",
      "Scoring responses:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Gets the response from llama index\n",
    "def get_llama_response(prompt):\n",
    "    response = query_engine.query(prompt)\n",
    "    context = [x.text for x in response.source_nodes]\n",
    "    return {\n",
    "        \"llm_answer\": response.response,\n",
    "        \"llm_context_list\": context\n",
    "    }\n",
    "\n",
    "\n",
    "# Score the responses\n",
    "print(\"Scoring responses...\")\n",
    "scorer = ValidateScorer(\n",
    "    [\n",
    "        AnswerSimilarityMetric(),\n",
    "        RetrievalPrecisionMetric(),\n",
    "        AugmentationPrecisionMetric(),\n",
    "        AugmentationAccuracyMetric(),\n",
    "        AnswerConsistencyMetric(),\n",
    "    ],\n",
    "    model_evaluator=\"gpt-3.5-turbo\",\n",
    "    fail_on_error=True,\n",
    ")\n",
    "run = scorer.score(benchmark, get_llama_response)\n",
    "validate_api = ValidateApi(TONIC_VALIDATE_API_KEY)\n",
    "validate_api.upload_run(\n",
    "    \"145ee5b0-4e7b-4abd-aeac-243b59274926\",\n",
    "    run,\n",
    "    run_metadata={\n",
    "        \"run_name\": \"Init Run\",\n",
    "        \"top_k\": 5,\n",
    "        \"summarized_top_k\": 10,\n",
    "    },\n",
    ")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from together import Together\n",
    "\n",
    "TOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")\n",
    "client = Together(api_key=TOGETHER_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"hello\"\n",
    "         }\n",
    "    ],\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\") \n",
    "client = OpenAI(api_key=api_key,)\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"hello\"}],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "chat_completion.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
